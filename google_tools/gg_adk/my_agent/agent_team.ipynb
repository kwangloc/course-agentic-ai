{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476b7feb",
   "metadata": {},
   "source": [
    "# 1/ Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90329d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "from typing import Optional, Dict, Any # For type hints\n",
    "\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.models.llm_request import LlmRequest\n",
    "from google.adk.models.llm_response import LlmResponse\n",
    "\n",
    "from google.adk.tools.base_tool import BaseTool\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c93b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n",
      "OpenAI API Key set: No (REPLACE PLACEHOLDER!)\n",
      "Anthropic API Key set: No (REPLACE PLACEHOLDER!)\n"
     ]
    }
   ],
   "source": [
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd804b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
    "MODEL_GEMINI_2_5_FLASH = \"gemini-2.5-flash\"\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
    "MODEL_GPT_4O = \"openai/gpt-4.1\" \n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\n",
    "MODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\"\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0548da",
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"weather_tutorial_agent_team\"\n",
    "USER_ID = \"user_1_agent_team\"\n",
    "SESSION_ID = \"session_001_agent_team\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f3cf2",
   "metadata": {},
   "source": [
    "# 2/ Define the tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b1a09",
   "metadata": {},
   "source": [
    "### get_weather_stateful()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5d35ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ State-aware 'get_weather_stateful' tool defined.\n"
     ]
    }
   ],
   "source": [
    "def get_weather_stateful(city: str, tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Retrieves weather, converts temp unit based on session state.\"\"\"\n",
    "    print(f\"--- Tool: get_weather_stateful called for {city} ---\")\n",
    "\n",
    "    # --- Read preference from state ---\n",
    "    preferred_unit = tool_context.state.get(\"user_preference_temperature_unit\", \"Celsius\") # Default to Celsius\n",
    "    print(f\"--- Tool: Reading state 'user_preference_temperature_unit': {preferred_unit} ---\")\n",
    "\n",
    "    city_normalized = city.lower().replace(\" \", \"\")\n",
    "\n",
    "    # Mock weather data (always stored in Celsius internally)\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"temp_c\": 25, \"condition\": \"sunny\"},\n",
    "        \"london\": {\"temp_c\": 15, \"condition\": \"cloudy\"},\n",
    "        \"tokyo\": {\"temp_c\": 18, \"condition\": \"light rain\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        data = mock_weather_db[city_normalized]\n",
    "        temp_c = data[\"temp_c\"]\n",
    "        condition = data[\"condition\"]\n",
    "\n",
    "        # Format temperature based on state preference\n",
    "        if preferred_unit == \"Fahrenheit\":\n",
    "            temp_value = (temp_c * 9/5) + 32 # Calculate Fahrenheit\n",
    "            temp_unit = \"°F\"\n",
    "        else: # Default to Celsius\n",
    "            temp_value = temp_c\n",
    "            temp_unit = \"°C\"\n",
    "\n",
    "        report = f\"The weather in {city.capitalize()} is {condition} with a temperature of {temp_value:.0f}{temp_unit}.\"\n",
    "        result = {\"status\": \"success\", \"report\": report}\n",
    "        print(f\"--- Tool: Generated report in {preferred_unit}. Result: {result} ---\")\n",
    "\n",
    "        # Example of writing back to state\n",
    "        tool_context.state[\"last_city_checked_stateful\"] = city\n",
    "        print(f\"--- Tool: Updated state 'last_city_checked_stateful': {city} ---\")\n",
    "\n",
    "        return result\n",
    "    else:\n",
    "        # Handle city not found\n",
    "        error_msg = f\"Sorry, I don't have weather information for '{city}'.\"\n",
    "        print(f\"--- Tool: City '{city}' not found. ---\")\n",
    "        return {\"status\": \"error\", \"error_message\": error_msg}\n",
    "\n",
    "print(\"✅ State-aware 'get_weather_stateful' tool defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72887b",
   "metadata": {},
   "source": [
    "### say_hello(), say_goodbye()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29197818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting and Farewell tools defined.\n"
     ]
    }
   ],
   "source": [
    "def say_hello(name: Optional[str] = None) -> str:\n",
    "    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\n",
    "\n",
    "    Args:\n",
    "        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\n",
    "\n",
    "    Returns:\n",
    "        str: A friendly greeting message.\n",
    "    \"\"\"\n",
    "    if name:\n",
    "        greeting = f\"Hello, {name}!\"\n",
    "        print(f\"--- Tool: say_hello called with name: {name} ---\")\n",
    "    else:\n",
    "        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\n",
    "        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\n",
    "    return greeting\n",
    "\n",
    "def say_goodbye() -> str:\n",
    "    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\n",
    "    print(f\"--- Tool: say_goodbye called ---\")\n",
    "    return \"Goodbye! Have a great day.\"\n",
    "\n",
    "print(\"Greeting and Farewell tools defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451a88a",
   "metadata": {},
   "source": [
    "# 3/ Define guardrails"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc5086",
   "metadata": {},
   "source": [
    "### block_keyword_guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c912cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ block_keyword_guardrail function defined.\n"
     ]
    }
   ],
   "source": [
    "def block_keyword_guardrail(\n",
    "    callback_context: CallbackContext, llm_request: LlmRequest\n",
    ") -> Optional[LlmResponse]:\n",
    "    \"\"\"\n",
    "    Inspects the latest user message for 'BLOCK'. If found, blocks the LLM call\n",
    "    and returns a predefined LlmResponse. Otherwise, returns None to proceed.\n",
    "    \"\"\"\n",
    "    agent_name = callback_context.agent_name # Get the name of the agent whose model call is being intercepted\n",
    "    print(f\"--- Callback: block_keyword_guardrail running for agent: {agent_name} ---\")\n",
    "\n",
    "    # Extract the text from the latest user message in the request history\n",
    "    last_user_message_text = \"\"\n",
    "    if llm_request.contents:\n",
    "        # Find the most recent message with role 'user'\n",
    "        for content in reversed(llm_request.contents):\n",
    "            if content.role == 'user' and content.parts:\n",
    "                # Assuming text is in the first part for simplicity\n",
    "                if content.parts[0].text:\n",
    "                    last_user_message_text = content.parts[0].text\n",
    "                    break # Found the last user message text\n",
    "\n",
    "    print(f\"--- Callback: Inspecting last user message: '{last_user_message_text[:100]}...' ---\") # Log first 100 chars\n",
    "\n",
    "    # --- Guardrail Logic ---\n",
    "    keyword_to_block = \"BLOCK\"\n",
    "    if keyword_to_block in last_user_message_text.upper(): # Case-insensitive check\n",
    "        print(f\"--- Callback: Found '{keyword_to_block}'. Blocking LLM call! ---\")\n",
    "        # Optionally, set a flag in state to record the block event\n",
    "        callback_context.state[\"guardrail_block_keyword_triggered\"] = True\n",
    "        print(f\"--- Callback: Set state 'guardrail_block_keyword_triggered': True ---\")\n",
    "\n",
    "        # Construct and return an LlmResponse to stop the flow and send this back instead\n",
    "        return LlmResponse(\n",
    "            content=types.Content(\n",
    "                role=\"model\", # Mimic a response from the agent's perspective\n",
    "                parts=[types.Part(text=f\"I cannot process this request because it contains the blocked keyword '{keyword_to_block}'.\")],\n",
    "            )\n",
    "            # Note: You could also set an error_message field here if needed\n",
    "        )\n",
    "    else:\n",
    "        # Keyword not found, allow the request to proceed to the LLM\n",
    "        print(f\"--- Callback: Keyword not found. Allowing LLM call for {agent_name}. ---\")\n",
    "        return None # Returning None signals ADK to continue normally\n",
    "\n",
    "print(\"✅ block_keyword_guardrail function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111b3134",
   "metadata": {},
   "source": [
    "### before_tool_callback - block_paris_tool_guardrail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a62fafb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ block_paris_tool_guardrail function defined.\n"
     ]
    }
   ],
   "source": [
    "def block_paris_tool_guardrail(\n",
    "    tool: BaseTool, args: Dict[str, Any], tool_context: ToolContext\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Checks if 'get_weather_stateful' is called for 'Paris'.\n",
    "    If so, blocks the tool execution and returns a specific error dictionary.\n",
    "    Otherwise, allows the tool call to proceed by returning None.\n",
    "    \"\"\"\n",
    "    tool_name = tool.name\n",
    "    agent_name = tool_context.agent_name # Agent attempting the tool call\n",
    "    print(f\"--- Callback: block_paris_tool_guardrail running for tool '{tool_name}' in agent '{agent_name}' ---\")\n",
    "    print(f\"--- Callback: Inspecting args: {args} ---\")\n",
    "\n",
    "    # --- Guardrail Logic ---\n",
    "    target_tool_name = \"get_weather_stateful\" # Match the function name used by FunctionTool\n",
    "    blocked_city = \"paris\"\n",
    "\n",
    "    # Check if it's the correct tool and the city argument matches the blocked city\n",
    "    if tool_name == target_tool_name:\n",
    "        city_argument = args.get(\"city\", \"\") # Safely get the 'city' argument\n",
    "        if city_argument and city_argument.lower() == blocked_city:\n",
    "            print(f\"--- Callback: Detected blocked city '{city_argument}'. Blocking tool execution! ---\")\n",
    "            # Optionally update state\n",
    "            tool_context.state[\"guardrail_tool_block_triggered\"] = True\n",
    "            print(f\"--- Callback: Set state 'guardrail_tool_block_triggered': True ---\")\n",
    "\n",
    "            # Return a dictionary matching the tool's expected output format for errors\n",
    "            # This dictionary becomes the tool's result, skipping the actual tool run.\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": f\"Policy restriction: Weather checks for '{city_argument.capitalize()}' are currently disabled by a tool guardrail.\"\n",
    "            }\n",
    "        else:\n",
    "             print(f\"--- Callback: City '{city_argument}' is allowed for tool '{tool_name}'. ---\")\n",
    "    else:\n",
    "        print(f\"--- Callback: Tool '{tool_name}' is not the target tool. Allowing. ---\")\n",
    "\n",
    "    # If the checks above didn't return a dictionary, allow the tool to execute\n",
    "    print(f\"--- Callback: Allowing tool '{tool_name}' to proceed. ---\")\n",
    "    return None # Returning None allows the actual tool function to run\n",
    "\n",
    "print(\"✅ block_paris_tool_guardrail function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6554bd",
   "metadata": {},
   "source": [
    "# 4/ Define the agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc963d",
   "metadata": {},
   "source": [
    "### Sub-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b28d9410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sub-Agent 'greeting_agent' redefined.\n"
     ]
    }
   ],
   "source": [
    "# --- Greeting Agent ---\n",
    "greeting_agent = None\n",
    "try:\n",
    "    # Use a defined model constant\n",
    "    greeting_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_5_FLASH,\n",
    "        name=\"greeting_agent\", # Keep original name for consistency\n",
    "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting using the 'say_hello' tool. Do nothing else.\",\n",
    "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\",\n",
    "        tools=[say_hello],\n",
    "    )\n",
    "    print(f\"✅ Sub-Agent '{greeting_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Greeting agent. Check Model/API Key ({greeting_agent.model}). Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c7b9596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sub-Agent 'farewell_agent' redefined.\n"
     ]
    }
   ],
   "source": [
    "# --- Farewell Agent ---\n",
    "farewell_agent = None\n",
    "try:\n",
    "    # Use a defined model constant\n",
    "    farewell_agent = Agent(\n",
    "        model=MODEL_GEMINI_2_5_FLASH,\n",
    "        name=\"farewell_agent\", # Keep original name\n",
    "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message using the 'say_goodbye' tool. Do not perform any other actions.\",\n",
    "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\",\n",
    "        tools=[say_goodbye],\n",
    "    )\n",
    "    print(f\"✅ Sub-Agent '{farewell_agent.name}' redefined.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not redefine Farewell agent. Check Model/API Key ({farewell_agent.model}). Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f24fc",
   "metadata": {},
   "source": [
    "### Root agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7990611e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root Agent 'weather_agent_v6_tool_guardrail' created with BOTH callbacks.\n"
     ]
    }
   ],
   "source": [
    "# --- Define the Root Agent with Both Callbacks ---\n",
    "root_agent_tool_guardrail = None\n",
    "\n",
    "if ('greeting_agent' in globals() and greeting_agent and\n",
    "    'farewell_agent' in globals() and farewell_agent and\n",
    "    'get_weather_stateful' in globals() and\n",
    "    'block_keyword_guardrail' in globals() and\n",
    "    'block_paris_tool_guardrail' in globals()):\n",
    "\n",
    "    root_agent_model = MODEL_GEMINI_2_5_FLASH\n",
    "\n",
    "    root_agent_tool_guardrail = Agent(\n",
    "        name=\"weather_agent_v6_tool_guardrail\", \n",
    "        model=root_agent_model,\n",
    "        description=\"Main agent: Handles weather, delegates, includes input AND tool guardrails.\",\n",
    "        instruction=\"You are the main Weather Agent. Provide weather using 'get_weather_stateful'. \"\n",
    "                    \"Delegate greetings to 'greeting_agent' and farewells to 'farewell_agent'. \"\n",
    "                    \"Handle only weather, greetings, and farewells.\",\n",
    "        tools=[get_weather_stateful],\n",
    "        sub_agents=[greeting_agent, farewell_agent],\n",
    "        output_key=\"last_weather_report\",\n",
    "        before_model_callback=block_keyword_guardrail, # model guardrail\n",
    "        before_tool_callback=block_paris_tool_guardrail # tool guardrail\n",
    "    )\n",
    "    print(f\"✅ Root Agent '{root_agent_tool_guardrail.name}' created with BOTH callbacks.\")\n",
    "else:\n",
    "    print(\"❌ Cannot create root agent with tool guardrail. Prerequisites missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a1202",
   "metadata": {},
   "source": [
    "# 5/ Runner and Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab04a2e8",
   "metadata": {},
   "source": [
    "### call_agent_async()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c5570eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "   \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "   print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "   # Prepare the user's message in ADK format\n",
    "   content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "   final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "   # Key Concept: run_async executes the agent logic and yields Events.\n",
    "   # We iterate through events to find the final answer.\n",
    "   async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "               # Assuming text response in the first part\n",
    "               final_response_text = event.content.parts[0].text\n",
    "            elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "               final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "            # Add more checks here if needed (e.g., specific error codes)\n",
    "            break # Stop processing events once the final response is found\n",
    "\n",
    "   print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b2498",
   "metadata": {},
   "source": [
    "### session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71fa6153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ New InMemorySessionService created for state demonstration.\n",
      "✅ Session 'session_001_agent_team' created for user 'user_1_agent_team'.\n",
      "\n",
      "--- Initial Session State ---\n",
      "{'user_preference_temperature_unit': 'Celsius'}\n"
     ]
    }
   ],
   "source": [
    "# Create a NEW session service instance for this state demonstration\n",
    "session_service_stateful = InMemorySessionService()\n",
    "print(\"✅ New InMemorySessionService created for state demonstration.\")\n",
    "\n",
    "# Define initial state data - user prefers Celsius initially\n",
    "initial_state = {\n",
    "    \"user_preference_temperature_unit\": \"Celsius\"\n",
    "}\n",
    "\n",
    "# Create the session, providing the initial state\n",
    "session_stateful = await session_service_stateful.create_session(\n",
    "    app_name=APP_NAME, # Use the consistent app name\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID,\n",
    "    state=initial_state # <<< Initialize state during creation\n",
    ")\n",
    "print(f\"✅ Session '{SESSION_ID}' created for user '{USER_ID}'.\")\n",
    "\n",
    "# Verify the initial state was set correctly\n",
    "retrieved_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
    "                                                         user_id=USER_ID,\n",
    "                                                         session_id = SESSION_ID)\n",
    "print(\"\\n--- Initial Session State ---\")\n",
    "if retrieved_session:\n",
    "    print(retrieved_session.state)\n",
    "else:\n",
    "    print(\"Error: Could not retrieve session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29da5e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_preference_temperature_unit': 'Celsius'}\n"
     ]
    }
   ],
   "source": [
    "print(session_stateful.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d7477",
   "metadata": {},
   "source": [
    "### runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b05e27e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Runner created for tool guardrail agent 'weather_agent_v6_tool_guardrail', using stateful session service.\n"
     ]
    }
   ],
   "source": [
    "runner_root_tool_guardrail = None\n",
    "\n",
    "if 'session_service_stateful' in globals():\n",
    "    runner_root_tool_guardrail = Runner(\n",
    "        agent=root_agent_tool_guardrail,\n",
    "        app_name=APP_NAME,\n",
    "        session_service=session_service_stateful # <<< Use the service from Step 4/5\n",
    "    )\n",
    "    print(f\"✅ Runner created for tool guardrail agent '{runner_root_tool_guardrail.agent.name}', using stateful session service.\")\n",
    "else:\n",
    "    print(\"❌ Cannot create runner. 'session_service_stateful' from Step 4/5 is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320eb120",
   "metadata": {},
   "source": [
    "# 6/ Interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aee3fa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting execution using 'await' (default for notebooks)...\n",
      "\n",
      "--- Testing Tool Argument Guardrail ('Paris' blocked) ---\n",
      "--- Turn 1: Requesting weather in New York (expect allowed) ---\n",
      "\n",
      ">>> User Query: What's the weather in New York?\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'What's the weather in New York?...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
      "--- Callback: Inspecting args: {'city': 'New York'} ---\n",
      "--- Callback: City 'New York' is allowed for tool 'get_weather_stateful'. ---\n",
      "--- Callback: Allowing tool 'get_weather_stateful' to proceed. ---\n",
      "--- Tool: get_weather_stateful called for New York ---\n",
      "--- Tool: Reading state 'user_preference_temperature_unit': Celsius ---\n",
      "--- Tool: Generated report in Celsius. Result: {'status': 'success', 'report': 'The weather in New york is sunny with a temperature of 25°C.'} ---\n",
      "--- Tool: Updated state 'last_city_checked_stateful': New York ---\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'What's the weather in New York?...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
      "\n",
      "--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'How about Paris?...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
      "--- Callback: Inspecting args: {'city': 'Paris'} ---\n",
      "--- Callback: Detected blocked city 'Paris'. Blocking tool execution! ---\n",
      "--- Callback: Set state 'guardrail_tool_block_triggered': True ---\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'How about Paris?...' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "<<< Agent Response: I'm sorry, I cannot provide weather information for Paris due to a policy restriction.\n",
      "\n",
      "--- Turn 3: Requesting weather in London (expect allowed) ---\n",
      "\n",
      ">>> User Query: Tell me the weather in London.\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'Tell me the weather in London....' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "--- Callback: block_paris_tool_guardrail running for tool 'get_weather_stateful' in agent 'weather_agent_v6_tool_guardrail' ---\n",
      "--- Callback: Inspecting args: {'city': 'London'} ---\n",
      "--- Callback: City 'London' is allowed for tool 'get_weather_stateful'. ---\n",
      "--- Callback: Allowing tool 'get_weather_stateful' to proceed. ---\n",
      "--- Tool: get_weather_stateful called for London ---\n",
      "--- Tool: Reading state 'user_preference_temperature_unit': Celsius ---\n",
      "--- Tool: Generated report in Celsius. Result: {'status': 'success', 'report': 'The weather in London is cloudy with a temperature of 15°C.'} ---\n",
      "--- Tool: Updated state 'last_city_checked_stateful': London ---\n",
      "--- Callback: block_keyword_guardrail running for agent: weather_agent_v6_tool_guardrail ---\n",
      "--- Callback: Inspecting last user message: 'Tell me the weather in London....' ---\n",
      "--- Callback: Keyword not found. Allowing LLM call for weather_agent_v6_tool_guardrail. ---\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 15°C.\n",
      "\n",
      "--- Inspecting Final Session State (After Tool Guardrail Test) ---\n",
      "Tool Guardrail Triggered Flag: True\n",
      "Last Weather Report: The weather in London is cloudy with a temperature of 15°C.\n",
      "Temperature Unit: Celsius\n"
     ]
    }
   ],
   "source": [
    "# Ensure the runner for the tool guardrail agent is available\n",
    "if 'runner_root_tool_guardrail' in globals() and runner_root_tool_guardrail:\n",
    "    # Define the main async function for the tool guardrail test conversation.\n",
    "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
    "    async def run_tool_guardrail_test():\n",
    "        print(\"\\n--- Testing Tool Argument Guardrail ('Paris' blocked) ---\")\n",
    "\n",
    "        # Use the runner for the agent with both callbacks and the existing stateful session\n",
    "        # Define a helper lambda for cleaner interaction calls\n",
    "        interaction_func = lambda query: call_agent_async(query,\n",
    "                                                         runner_root_tool_guardrail,\n",
    "                                                         USER_ID, # Use existing user ID\n",
    "                                                         SESSION_ID # Use existing session ID\n",
    "                                                        )\n",
    "        # 1. Allowed city (Should pass both callbacks, use Fahrenheit state)\n",
    "        print(\"--- Turn 1: Requesting weather in New York (expect allowed) ---\")\n",
    "        await interaction_func(\"What's the weather in New York?\")\n",
    "\n",
    "        # 2. Blocked city (Should pass model callback, but be blocked by tool callback)\n",
    "        print(\"\\n--- Turn 2: Requesting weather in Paris (expect blocked by tool guardrail) ---\")\n",
    "        await interaction_func(\"How about Paris?\") # Tool callback should intercept this\n",
    "\n",
    "        # 3. Another allowed city (Should work normally again)\n",
    "        print(\"\\n--- Turn 3: Requesting weather in London (expect allowed) ---\")\n",
    "        await interaction_func(\"Tell me the weather in London.\")\n",
    "\n",
    "    # --- Execute the `run_tool_guardrail_test` async function ---\n",
    "    # Choose ONE of the methods below based on your environment.\n",
    "\n",
    "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
    "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
    "    # it means an event loop is already running, so you can directly await the function.\n",
    "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
    "    await run_tool_guardrail_test()\n",
    "\n",
    "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
    "    # If running this code as a standard Python script from your terminal,\n",
    "    # the script context is synchronous. `asyncio.run()` is needed to\n",
    "    # create and manage an event loop to execute your async function.\n",
    "    # To use this method:\n",
    "    # 1. Comment out the `await run_tool_guardrail_test()` line above.\n",
    "    # 2. Uncomment the following block:\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
    "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
    "        try:\n",
    "            # This creates an event loop, runs your async function, and closes the loop.\n",
    "            asyncio.run(run_tool_guardrail_test())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Inspect final session state after the conversation ---\n",
    "    # This block runs after either execution method completes.\n",
    "    # Optional: Check state for the tool block trigger flag\n",
    "    print(\"\\n--- Inspecting Final Session State (After Tool Guardrail Test) ---\")\n",
    "    # Use the session service instance associated with this stateful session\n",
    "    final_session = await session_service_stateful.get_session(app_name=APP_NAME,\n",
    "                                                         user_id=USER_ID,\n",
    "                                                         session_id= SESSION_ID)\n",
    "    if final_session:\n",
    "        # Use .get() for safer access\n",
    "        print(f\"Tool Guardrail Triggered Flag: {final_session.state.get('guardrail_tool_block_triggered', 'Not Set (or False)')}\")\n",
    "        print(f\"Last Weather Report: {final_session.state.get('last_weather_report', 'Not Set')}\") # Should be London weather if successful\n",
    "        print(f\"Temperature Unit: {final_session.state.get('user_preference_temperature_unit', 'Not Set')}\") # Should be Fahrenheit\n",
    "        # print(f\"Full State Dict: {final_session.state}\") # For detailed view\n",
    "    else:\n",
    "        print(\"\\n❌ Error: Could not retrieve final session state.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n⚠️ Skipping tool guardrail test. Runner ('runner_root_tool_guardrail') is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5236aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_preference_temperature_unit': 'Celsius',\n",
       " 'last_city_checked_stateful': 'London',\n",
       " 'last_weather_report': 'The weather in London is cloudy with a temperature of 15°C.',\n",
       " 'guardrail_tool_block_triggered': True}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_session.state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_agentic_ai_venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
