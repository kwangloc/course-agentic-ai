{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4fea62c",
   "metadata": {},
   "source": [
    "- Tool Definition & Usage\n",
    "- Multi-LLM Flexibility\n",
    "- Agent Delegation & Collaboration\n",
    "- Session State for Memory\n",
    "- Safety Guardrails with Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Import necessary libraries\n",
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e645dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n",
      "OpenAI API Key set: No (REPLACE PLACEHOLDER!)\n",
      "Anthropic API Key set: No (REPLACE PLACEHOLDER!)\n"
     ]
    }
   ],
   "source": [
    "# @title Configure API Keys \n",
    "\n",
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0fa94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "\n",
    "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
    "MODEL_GEMINI_2_5_FLASH = \"gemini-2.5-flash\"\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
    "MODEL_GPT_4O = \"openai/gpt-4.1\" # You can also try: gpt-4.1-mini, gpt-4o etc.\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\n",
    "MODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bdc8cd",
   "metadata": {},
   "source": [
    "# Step 1: Your First Agent - Basic Weather Lookup¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5d54cf",
   "metadata": {},
   "source": [
    "### 1. Define the Tool (get_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d45b1404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfedd47",
   "metadata": {},
   "source": [
    "### 2. Define the Agent (weather_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caab4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.5-flash'.\n"
     ]
    }
   ],
   "source": [
    "AGENT_MODEL = MODEL_GEMINI_2_5_FLASH # Starting with Gemini\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b8400",
   "metadata": {},
   "source": [
    "### 3. Setup Runner and Session Service\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58136310",
   "metadata": {},
   "source": [
    "- Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f6079f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n"
     ]
    }
   ],
   "source": [
    "# --- Session Management ---\n",
    "# Key Concept: SessionService stores conversation history & state.\n",
    "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f340ec",
   "metadata": {},
   "source": [
    "- Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8bb9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c78ed",
   "metadata": {},
   "source": [
    "### 4. Interact with the Agent\n",
    "- a way to send messages to our agent and receive its responses.\n",
    "- asynchronously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae4e38",
   "metadata": {},
   "source": [
    "- Helper function\n",
    "    - 1/ Takes a user query string.\n",
    "    - 2/ Packages it into the ADK Content format.\n",
    "    - 3/ Calls runner.run_async, providing the user/session context and the new message.\n",
    "    - 4/ Iterates through the Events yielded by the runner. Events represent steps in the agent's execution (e.g., tool call requested, tool result received, intermediate LLM thought, final response).\n",
    "    - 5/ Identifies and prints the final response event using event.is_final_response()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1725a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563e3a2",
   "metadata": {},
   "source": [
    "### 5. Run the Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c805d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is the weather like in London?\n",
      "--- Tool: get_weather called for city: London ---\n",
      "<<< Agent Response: It's cloudy in London with a temperature of 15°C.\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "<<< Agent Response: Sorry, I don't have weather information for 'Paris'.\n",
      "\n",
      ">>> User Query: Tell me the weather in New York\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n"
     ]
    }
   ],
   "source": [
    "# @title Run the Initial Conversation\n",
    "\n",
    "# We need an async function to await our interaction helper\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\"What is the weather like in London?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "    await call_agent_async(\"How about Paris?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
    "\n",
    "    await call_agent_async(\"Tell me the weather in New York\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa6de4",
   "metadata": {},
   "source": [
    "# Step 3: Building an Agent Team - Delegation for Greetings & Farewells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8947ca77",
   "metadata": {},
   "source": [
    "#### 1. Define Tools for Sub-Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f48b696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting and Farewell tools defined.\n",
      "--- Tool: say_hello called with name: Alice ---\n",
      "Hello, Alice!\n",
      "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
      "Hello there!\n",
      "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
      "Hello there!\n"
     ]
    }
   ],
   "source": [
    "# @title Define Tools for Greeting and Farewell Agents\n",
    "from typing import Optional # Make sure to import Optional\n",
    "\n",
    "# Ensure 'get_weather' from Step 1 is available if running this step independently.\n",
    "# def get_weather(city: str) -> dict: ... (from Step 1)\n",
    "\n",
    "def say_hello(name: Optional[str] = None) -> str:\n",
    "    \"\"\"Provides a simple greeting. If a name is provided, it will be used.\n",
    "\n",
    "    Args:\n",
    "        name (str, optional): The name of the person to greet. Defaults to a generic greeting if not provided.\n",
    "\n",
    "    Returns:\n",
    "        str: A friendly greeting message.\n",
    "    \"\"\"\n",
    "    if name:\n",
    "        greeting = f\"Hello, {name}!\"\n",
    "        print(f\"--- Tool: say_hello called with name: {name} ---\")\n",
    "    else:\n",
    "        greeting = \"Hello there!\" # Default greeting if name is None or not explicitly passed\n",
    "        print(f\"--- Tool: say_hello called without a specific name (name_arg_value: {name}) ---\")\n",
    "    return greeting\n",
    "\n",
    "def say_goodbye() -> str:\n",
    "    \"\"\"Provides a simple farewell message to conclude the conversation.\"\"\"\n",
    "    print(f\"--- Tool: say_goodbye called ---\")\n",
    "    return \"Goodbye! Have a great day.\"\n",
    "\n",
    "print(\"Greeting and Farewell tools defined.\")\n",
    "\n",
    "# Optional self-test\n",
    "print(say_hello(\"Alice\"))\n",
    "print(say_hello()) # Test with no argument (should use default \"Hello there!\")\n",
    "print(say_hello(name=None)) # Test with name explicitly as None (should use default \"Hello there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb8cbc",
   "metadata": {},
   "source": [
    "#### 2. Define the Sub-Agents (Greeting & Farewell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c429253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Agent 'greeting_agent' created using model 'gemini-2.5-flash'.\n",
      "✅ Agent 'farewell_agent' created using model 'gemini-2.5-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define Greeting and Farewell Sub-Agents\n",
    "\n",
    "# If you want to use models other than Gemini, Ensure LiteLlm is imported and API keys are set (from Step 0/2)\n",
    "# from google.adk.models.lite_llm import LiteLlm\n",
    "# MODEL_GPT_4O, MODEL_CLAUDE_SONNET etc. should be defined\n",
    "# Or else, continue to use: model = MODEL_GEMINI_2_5_FLASH\n",
    "\n",
    "# --- Greeting Agent ---\n",
    "greeting_agent = None\n",
    "try:\n",
    "    greeting_agent = Agent(\n",
    "        # Using a potentially different/cheaper model for a simple task\n",
    "        model = MODEL_GEMINI_2_5_FLASH,\n",
    "        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n",
    "        name=\"greeting_agent\",\n",
    "        instruction=\"You are the Greeting Agent. Your ONLY task is to provide a friendly greeting to the user. \"\n",
    "                    \"Use the 'say_hello' tool to generate the greeting. \"\n",
    "                    \"If the user provides their name, make sure to pass it to the tool. \"\n",
    "                    \"Do not engage in any other conversation or tasks.\",\n",
    "        description=\"Handles simple greetings and hellos using the 'say_hello' tool.\", # Crucial for delegation\n",
    "        tools=[say_hello],\n",
    "    )\n",
    "    print(f\"✅ Agent '{greeting_agent.name}' created using model '{greeting_agent.model}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create Greeting agent. Check API Key ({greeting_agent.model}). Error: {e}\")\n",
    "\n",
    "# --- Farewell Agent ---\n",
    "farewell_agent = None\n",
    "try:\n",
    "    farewell_agent = Agent(\n",
    "        # Can use the same or a different model\n",
    "        model = MODEL_GEMINI_2_5_FLASH,\n",
    "        # model=LiteLlm(model=MODEL_GPT_4O), # If you would like to experiment with other models\n",
    "        name=\"farewell_agent\",\n",
    "        instruction=\"You are the Farewell Agent. Your ONLY task is to provide a polite goodbye message. \"\n",
    "                    \"Use the 'say_goodbye' tool when the user indicates they are leaving or ending the conversation \"\n",
    "                    \"(e.g., using words like 'bye', 'goodbye', 'thanks bye', 'see you'). \"\n",
    "                    \"Do not perform any other actions.\",\n",
    "        description=\"Handles simple farewells and goodbyes using the 'say_goodbye' tool.\", # Crucial for delegation\n",
    "        tools=[say_goodbye],\n",
    "    )\n",
    "    print(f\"✅ Agent '{farewell_agent.name}' created using model '{farewell_agent.model}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create Farewell agent. Check API Key ({farewell_agent.model}). Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bab481",
   "metadata": {},
   "source": [
    "#### 3. Define the Root Agent (Weather Agent v2) with Sub-Agents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1c17b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Root Agent 'weather_agent_v2' created using model 'gemini-2.5-flash' with sub-agents: ['greeting_agent', 'farewell_agent']\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Root Agent with Sub-Agents\n",
    "\n",
    "# Ensure sub-agents were created successfully before defining the root agent.\n",
    "# Also ensure the original 'get_weather' tool is defined.\n",
    "root_agent = None\n",
    "runner_root = None # Initialize runner\n",
    "\n",
    "if greeting_agent and farewell_agent and 'get_weather' in globals():\n",
    "    # Let's use a capable Gemini model for the root agent to handle orchestration\n",
    "    root_agent_model = MODEL_GEMINI_2_5_FLASH\n",
    "\n",
    "    weather_agent_team = Agent(\n",
    "        name=\"weather_agent_v2\", # Give it a new version name\n",
    "        model=root_agent_model,\n",
    "        description=\"The main coordinator agent. Handles weather requests and delegates greetings/farewells to specialists.\",\n",
    "        instruction=\"You are the main Weather Agent coordinating a team. Your primary responsibility is to provide weather information. \"\n",
    "                    \"Use the 'get_weather' tool ONLY for specific weather requests (e.g., 'weather in London'). \"\n",
    "                    \"You have specialized sub-agents: \"\n",
    "                    \"1. 'greeting_agent': Handles simple greetings like 'Hi', 'Hello'. Delegate to it for these. \"\n",
    "                    \"2. 'farewell_agent': Handles simple farewells like 'Bye', 'See you'. Delegate to it for these. \"\n",
    "                    \"Analyze the user's query. If it's a greeting, delegate to 'greeting_agent'. If it's a farewell, delegate to 'farewell_agent'. \"\n",
    "                    \"If it's a weather request, handle it yourself using 'get_weather'. \"\n",
    "                    \"For anything else, respond appropriately or state you cannot handle it.\",\n",
    "        tools=[get_weather], # Root agent still needs the weather tool for its core task\n",
    "        # Key change: Link the sub-agents here!\n",
    "        sub_agents=[greeting_agent, farewell_agent]\n",
    "    )\n",
    "    print(f\"✅ Root Agent '{weather_agent_team.name}' created using model '{root_agent_model}' with sub-agents: {[sa.name for sa in weather_agent_team.sub_agents]}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize or 'get_weather' tool is missing.\")\n",
    "    if not greeting_agent: print(\" - Greeting Agent is missing.\")\n",
    "    if not farewell_agent: print(\" - Farewell Agent is missing.\")\n",
    "    if 'get_weather' not in globals(): print(\" - get_weather function is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a1001",
   "metadata": {},
   "source": [
    "#### 4. Interact with the Agent Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7768a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting execution using 'await' (default for notebooks)...\n",
      "\n",
      "--- Testing Agent Team Delegation ---\n",
      "Session created: App='weather_tutorial_agent_team', User='user_1_agent_team', Session='session_001_agent_team'\n",
      "Runner created for agent 'weather_agent_v2'.\n",
      "\n",
      ">>> User Query: Hello there!\n",
      "--- Tool: say_hello called without a specific name (name_arg_value: None) ---\n",
      "<<< Agent Response: Hello there!\n",
      "\n",
      "\n",
      ">>> User Query: What is the weather in New York?\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25°C.\n",
      "\n",
      ">>> User Query: Thanks, bye!\n"
     ]
    },
    {
     "ename": "_ResourceExhaustedError",
     "evalue": "\nOn how to mitigate this issue, please refer to:\n\nhttps://google.github.io/adk-docs/agents/models/#error-code-429-resource_exhausted\n\n\n429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 11.02309381s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '11s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/models/google_llm.py:241\u001b[39m, in \u001b[36mGemini.generate_content_async\u001b[39m\u001b[34m(self, llm_request, stream)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m   response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_client.aio.models.generate_content(\n\u001b[32m    242\u001b[39m       model=llm_request.model,\n\u001b[32m    243\u001b[39m       contents=llm_request.contents,\n\u001b[32m    244\u001b[39m       config=llm_request.config,\n\u001b[32m    245\u001b[39m   )\n\u001b[32m    246\u001b[39m   logger.info(\u001b[33m'\u001b[39m\u001b[33mResponse received from the model.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/genai/models.py:7018\u001b[39m, in \u001b[36mAsyncModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   7011\u001b[39m     logger.warning(\n\u001b[32m   7012\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mTools at indices [\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m] are not compatible with automatic function \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   7013\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcalling (AFC). AFC is disabled. If AFC is intended, please \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   7016\u001b[39m         indices_str,\n\u001b[32m   7017\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m7018\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_content(\n\u001b[32m   7019\u001b[39m       model=model, contents=contents, config=parsed_config\n\u001b[32m   7020\u001b[39m   )\n\u001b[32m   7021\u001b[39m remaining_remote_calls_afc = _extra_utils.get_max_remote_calls_afc(\n\u001b[32m   7022\u001b[39m     parsed_config\n\u001b[32m   7023\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/genai/models.py:5836\u001b[39m, in \u001b[36mAsyncModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5834\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m5836\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.async_request(\n\u001b[32m   5837\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m'\u001b[39m, path, request_dict, http_options\n\u001b[32m   5838\u001b[39m )\n\u001b[32m   5840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   5841\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   5842\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/genai/_api_client.py:1421\u001b[39m, in \u001b[36mBaseApiClient.async_request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1417\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1418\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1419\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1421\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_request(\n\u001b[32m   1422\u001b[39m     http_request=http_request, http_options=http_options, stream=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1423\u001b[39m )\n\u001b[32m   1424\u001b[39m response_body = result.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m result.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/genai/_api_client.py:1354\u001b[39m, in \u001b[36mBaseApiClient._async_request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._async_request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_retry(  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1355\u001b[39m     \u001b[38;5;28mself\u001b[39m._async_request_once, http_request, stream\n\u001b[32m   1356\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.3/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/tenacity/asyncio/__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/genai/_api_client.py:1299\u001b[39m, in \u001b[36mBaseApiClient._async_request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1291\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiohttp_session.request(\n\u001b[32m   1292\u001b[39m     method=http_request.method,\n\u001b[32m   1293\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1297\u001b[39m     **\u001b[38;5;28mself\u001b[39m._async_client_session_request_args,\n\u001b[32m   1298\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1299\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m errors.APIError.raise_for_async_response(response)\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(response.headers, [\u001b[38;5;28;01mawait\u001b[39;00m response.text()])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/genai/errors.py:203\u001b[39m, in \u001b[36mAPIError.raise_for_async_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mUnsupported response type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.raise_error_async(status_code, response_json, response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/genai/errors.py:225\u001b[39m, in \u001b[36mAPIError.raise_error_async\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 11.02309381s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '11s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31m_ResourceExhaustedError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# --- Execute the `run_team_conversation` async function ---\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Choose ONE of the methods below based on your environment.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Note: This may require API keys for the models used!\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# If your environment supports top-level await (like Colab/Jupyter notebooks),\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# it means an event loop is already running, so you can directly await the function.\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAttempting execution using \u001b[39m\u001b[33m'\u001b[39m\u001b[33mawait\u001b[39m\u001b[33m'\u001b[39m\u001b[33m (default for notebooks)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_team_conversation()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# METHOD 2: asyncio.run (For Standard Python Scripts [.py])\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# If running this code as a standard Python script from your terminal,\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# the script context is synchronous. `asyncio.run()` is needed to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# 1. Comment out the `await run_team_conversation()` line above.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# 2. Uncomment the following block:\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03mimport asyncio\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03mif __name__ == \"__main__\": # Ensures this runs only when script is executed directly\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m \u001b[33;03m        print(f\"An error occurred: {e}\")\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_team_conversation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m call_agent_async(query = \u001b[33m\"\u001b[39m\u001b[33mHello there!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m                        runner=runner_agent_team,\n\u001b[32m     42\u001b[39m                        user_id=USER_ID,\n\u001b[32m     43\u001b[39m                        session_id=SESSION_ID)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m call_agent_async(query = \u001b[33m\"\u001b[39m\u001b[33mWhat is the weather in New York?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m                        runner=runner_agent_team,\n\u001b[32m     46\u001b[39m                        user_id=USER_ID,\n\u001b[32m     47\u001b[39m                        session_id=SESSION_ID)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m call_agent_async(query = \u001b[33m\"\u001b[39m\u001b[33mThanks, bye!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     49\u001b[39m                        runner=runner_agent_team,\n\u001b[32m     50\u001b[39m                        user_id=USER_ID,\n\u001b[32m     51\u001b[39m                        session_id=SESSION_ID)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcall_agent_async\u001b[39m\u001b[34m(query, runner, user_id, session_id)\u001b[39m\n\u001b[32m     12\u001b[39m final_response_text = \u001b[33m\"\u001b[39m\u001b[33mAgent did not produce a final response.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# Default\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Key Concept: run_async executes the agent logic and yields Events.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# We iterate through events to find the final answer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# You can uncomment the line below to see *all* events during execution\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Key Concept: is_final_response() marks the concluding message for the turn.\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m event.is_final_response():\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m event.content \u001b[38;5;129;01mand\u001b[39;00m event.content.parts:\n\u001b[32m     23\u001b[39m            \u001b[38;5;66;03m# Assuming text response in the first part\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/runners.py:519\u001b[39m, in \u001b[36mRunner.run_async\u001b[39m\u001b[34m(self, user_id, session_id, invocation_id, new_message, state_delta, run_config)\u001b[39m\n\u001b[32m    514\u001b[39m       \u001b[38;5;28;01mawait\u001b[39;00m _run_compaction_for_sliding_window(\n\u001b[32m    515\u001b[39m           \u001b[38;5;28mself\u001b[39m.app, session, \u001b[38;5;28mself\u001b[39m.session_service\n\u001b[32m    516\u001b[39m       )\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(_run_with_trace(new_message, invocation_id)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    520\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/runners.py:507\u001b[39m, in \u001b[36mRunner.run_async.<locals>._run_with_trace\u001b[39m\u001b[34m(new_message, invocation_id)\u001b[39m\n\u001b[32m    497\u001b[39m       \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\n\u001b[32m    500\u001b[39m     \u001b[38;5;28mself\u001b[39m._exec_with_plugin(\n\u001b[32m    501\u001b[39m         invocation_context=invocation_context,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m     )\n\u001b[32m    506\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    509\u001b[39m \u001b[38;5;66;03m# Run compaction after all events are yielded from the agent.\u001b[39;00m\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# (We don't compact in the middle of an invocation, we only compact at\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[38;5;66;03m# the end of an invocation.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/runners.py:739\u001b[39m, in \u001b[36mRunner._exec_with_plugin\u001b[39m\u001b[34m(self, invocation_context, session, execute_fn, is_live_call)\u001b[39m\n\u001b[32m    736\u001b[39m is_transcribing: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(execute_fn(invocation_context)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    740\u001b[39m     _apply_run_config_custom_metadata(\n\u001b[32m    741\u001b[39m         event, invocation_context.run_config\n\u001b[32m    742\u001b[39m     )\n\u001b[32m    743\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_live_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/runners.py:496\u001b[39m, in \u001b[36mRunner.run_async.<locals>._run_with_trace.<locals>.execute\u001b[39m\u001b[34m(ctx)\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(ctx: InvocationContext) -> AsyncGenerator[Event]:\n\u001b[32m    495\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(ctx.agent.run_async(ctx)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    497\u001b[39m       \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py:294\u001b[39m, in \u001b[36mBaseAgent.run_async\u001b[39m\u001b[34m(self, parent_context)\u001b[39m\n\u001b[32m    291\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\u001b[38;5;28mself\u001b[39m._run_async_impl(ctx)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ctx.end_invocation:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/agents/llm_agent.py:468\u001b[39m, in \u001b[36mLlmAgent._run_async_impl\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    466\u001b[39m should_pause = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\u001b[38;5;28mself\u001b[39m._llm_flow.run_async(ctx)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m.__maybe_save_output_to_state(event)\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:362\u001b[39m, in \u001b[36mBaseLlmFlow.run_async\u001b[39m\u001b[34m(self, invocation_context)\u001b[39m\n\u001b[32m    360\u001b[39m last_event = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\u001b[38;5;28mself\u001b[39m._run_one_step_async(invocation_context)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    363\u001b[39m     last_event = event\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:449\u001b[39m, in \u001b[36mBaseLlmFlow._run_one_step_async\u001b[39m\u001b[34m(self, invocation_context)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m llm_response \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    440\u001b[39m   \u001b[38;5;66;03m# Postprocess after calling the LLM.\u001b[39;00m\n\u001b[32m    441\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\n\u001b[32m    442\u001b[39m       \u001b[38;5;28mself\u001b[39m._postprocess_async(\n\u001b[32m    443\u001b[39m           invocation_context,\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m       )\n\u001b[32m    448\u001b[39m   ) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    450\u001b[39m       \u001b[38;5;66;03m# Update the mutable event id to avoid conflict\u001b[39;00m\n\u001b[32m    451\u001b[39m       model_response_event.id = Event.new_id()\n\u001b[32m    452\u001b[39m       model_response_event.timestamp = datetime.datetime.now().timestamp()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:565\u001b[39m, in \u001b[36mBaseLlmFlow._postprocess_async\u001b[39m\u001b[34m(self, invocation_context, llm_request, llm_response, model_response_event)\u001b[39m\n\u001b[32m    558\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\n\u001b[32m    561\u001b[39m     \u001b[38;5;28mself\u001b[39m._postprocess_handle_function_calls_async(\n\u001b[32m    562\u001b[39m         invocation_context, model_response_event, llm_request\n\u001b[32m    563\u001b[39m     )\n\u001b[32m    564\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:712\u001b[39m, in \u001b[36mBaseLlmFlow._postprocess_handle_function_calls_async\u001b[39m\u001b[34m(self, invocation_context, function_call_event, llm_request)\u001b[39m\n\u001b[32m    708\u001b[39m agent_to_run = \u001b[38;5;28mself\u001b[39m._get_agent_to_run(\n\u001b[32m    709\u001b[39m     invocation_context, transfer_to_agent\n\u001b[32m    710\u001b[39m )\n\u001b[32m    711\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(agent_to_run.run_async(invocation_context)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m712\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/agents/base_agent.py:294\u001b[39m, in \u001b[36mBaseAgent.run_async\u001b[39m\u001b[34m(self, parent_context)\u001b[39m\n\u001b[32m    291\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\u001b[38;5;28mself\u001b[39m._run_async_impl(ctx)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    295\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ctx.end_invocation:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/agents/llm_agent.py:468\u001b[39m, in \u001b[36mLlmAgent._run_async_impl\u001b[39m\u001b[34m(self, ctx)\u001b[39m\n\u001b[32m    466\u001b[39m should_pause = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\u001b[38;5;28mself\u001b[39m._llm_flow.run_async(ctx)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m.__maybe_save_output_to_state(event)\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:362\u001b[39m, in \u001b[36mBaseLlmFlow.run_async\u001b[39m\u001b[34m(self, invocation_context)\u001b[39m\n\u001b[32m    360\u001b[39m last_event = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\u001b[38;5;28mself\u001b[39m._run_one_step_async(invocation_context)) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    363\u001b[39m     last_event = event\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:439\u001b[39m, in \u001b[36mBaseLlmFlow._run_one_step_async\u001b[39m\u001b[34m(self, invocation_context)\u001b[39m\n\u001b[32m    428\u001b[39m model_response_event = Event(\n\u001b[32m    429\u001b[39m     \u001b[38;5;28mid\u001b[39m=Event.new_id(),\n\u001b[32m    430\u001b[39m     invocation_id=invocation_context.invocation_id,\n\u001b[32m    431\u001b[39m     author=invocation_context.agent.name,\n\u001b[32m    432\u001b[39m     branch=invocation_context.branch,\n\u001b[32m    433\u001b[39m )\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\n\u001b[32m    435\u001b[39m     \u001b[38;5;28mself\u001b[39m._call_llm_async(\n\u001b[32m    436\u001b[39m         invocation_context, llm_request, model_response_event\n\u001b[32m    437\u001b[39m     )\n\u001b[32m    438\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m llm_response \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    440\u001b[39m     \u001b[38;5;66;03m# Postprocess after calling the LLM.\u001b[39;00m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\n\u001b[32m    442\u001b[39m         \u001b[38;5;28mself\u001b[39m._postprocess_async(\n\u001b[32m    443\u001b[39m             invocation_context,\n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m         )\n\u001b[32m    448\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m    449\u001b[39m       \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    450\u001b[39m         \u001b[38;5;66;03m# Update the mutable event id to avoid conflict\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:812\u001b[39m, in \u001b[36mBaseLlmFlow._call_llm_async\u001b[39m\u001b[34m(self, invocation_context, llm_request, model_response_event)\u001b[39m\n\u001b[32m    809\u001b[39m           \u001b[38;5;28;01myield\u001b[39;00m llm_response\n\u001b[32m    811\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(_call_llm_with_tracing()) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m812\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:796\u001b[39m, in \u001b[36mBaseLlmFlow._call_llm_async.<locals>._call_llm_with_tracing\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    783\u001b[39m responses_generator = llm.generate_content_async(\n\u001b[32m    784\u001b[39m     llm_request,\n\u001b[32m    785\u001b[39m     stream=invocation_context.run_config.streaming_mode\n\u001b[32m    786\u001b[39m     == StreamingMode.SSE,\n\u001b[32m    787\u001b[39m )\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(\n\u001b[32m    789\u001b[39m     \u001b[38;5;28mself\u001b[39m._run_and_handle_error(\n\u001b[32m    790\u001b[39m         responses_generator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    794\u001b[39m     )\n\u001b[32m    795\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m llm_response \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m    797\u001b[39m     trace_call_llm(\n\u001b[32m    798\u001b[39m         invocation_context,\n\u001b[32m    799\u001b[39m         model_response_event.id,\n\u001b[32m    800\u001b[39m         llm_request,\n\u001b[32m    801\u001b[39m         llm_response,\n\u001b[32m    802\u001b[39m     )\n\u001b[32m    803\u001b[39m     \u001b[38;5;66;03m# Runs after_model_callback if it exists.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:1049\u001b[39m, in \u001b[36mBaseLlmFlow._run_and_handle_error\u001b[39m\u001b[34m(self, response_generator, invocation_context, llm_request, model_response_event)\u001b[39m\n\u001b[32m   1047\u001b[39m   \u001b[38;5;28;01myield\u001b[39;00m error_response\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1049\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m model_error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/flows/llm_flows/base_llm_flow.py:1035\u001b[39m, in \u001b[36mBaseLlmFlow._run_and_handle_error\u001b[39m\u001b[34m(self, response_generator, invocation_context, llm_request, model_response_event)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1034\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Aclosing(response_generator) \u001b[38;5;28;01mas\u001b[39;00m agen:\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m agen:\n\u001b[32m   1036\u001b[39m       \u001b[38;5;28;01myield\u001b[39;00m response\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m model_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Self-learn/AIAgents/course-agentic-ai/course_agentic_ai_venv/lib/python3.12/site-packages/google/adk/models/google_llm.py:260\u001b[39m, in \u001b[36mGemini.generate_content_async\u001b[39m\u001b[34m(self, llm_request, stream)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m ce:\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m ce.code == \u001b[32m429\u001b[39m:\n\u001b[32m    257\u001b[39m     \u001b[38;5;66;03m# We expect running into a Resource Exhausted error to be a common\u001b[39;00m\n\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# client error that developers would run into. We enhance the messaging\u001b[39;00m\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# with possible fixes to this issue.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _ResourceExhaustedError(ce) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mce\u001b[39;00m\n\u001b[32m    262\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ce\n",
      "\u001b[31m_ResourceExhaustedError\u001b[39m: \nOn how to mitigate this issue, please refer to:\n\nhttps://google.github.io/adk-docs/agents/models/#error-code-429-resource_exhausted\n\n\n429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\\nPlease retry in 11.02309381s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '5'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '11s'}]}}"
     ]
    }
   ],
   "source": [
    "# @title Interact with the Agent Team\n",
    "import asyncio # Ensure asyncio is imported\n",
    "\n",
    "# Ensure the root agent (e.g., 'weather_agent_team' or 'root_agent' from the previous cell) is defined.\n",
    "# Ensure the call_agent_async function is defined.\n",
    "\n",
    "# Check if the root agent variable exists before defining the conversation function\n",
    "root_agent_var_name = 'root_agent' # Default name from Step 3 guide\n",
    "if 'weather_agent_team' in globals(): # Check if user used this name instead\n",
    "    root_agent_var_name = 'weather_agent_team'\n",
    "elif 'root_agent' not in globals():\n",
    "    print(\"⚠️ Root agent ('root_agent' or 'weather_agent_team') not found. Cannot define run_team_conversation.\")\n",
    "    # Assign a dummy value to prevent NameError later if the code block runs anyway\n",
    "    root_agent = None # Or set a flag to prevent execution\n",
    "\n",
    "# Only define and run if the root agent exists\n",
    "if root_agent_var_name in globals() and globals()[root_agent_var_name]:\n",
    "    # Define the main async function for the conversation logic.\n",
    "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
    "    async def run_team_conversation():\n",
    "        print(\"\\n--- Testing Agent Team Delegation ---\")\n",
    "        session_service = InMemorySessionService()\n",
    "        APP_NAME = \"weather_tutorial_agent_team\"\n",
    "        USER_ID = \"user_1_agent_team\"\n",
    "        SESSION_ID = \"session_001_agent_team\"\n",
    "        session = await session_service.create_session(\n",
    "            app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
    "        )\n",
    "        print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "        actual_root_agent = globals()[root_agent_var_name]\n",
    "        runner_agent_team = Runner( # Or use InMemoryRunner\n",
    "            agent=actual_root_agent,\n",
    "            app_name=APP_NAME,\n",
    "            session_service=session_service\n",
    "        )\n",
    "        print(f\"Runner created for agent '{actual_root_agent.name}'.\")\n",
    "\n",
    "        # --- Interactions using await (correct within async def) ---\n",
    "        await call_agent_async(query = \"Hello there!\",\n",
    "                               runner=runner_agent_team,\n",
    "                               user_id=USER_ID,\n",
    "                               session_id=SESSION_ID)\n",
    "        await call_agent_async(query = \"What is the weather in New York?\",\n",
    "                               runner=runner_agent_team,\n",
    "                               user_id=USER_ID,\n",
    "                               session_id=SESSION_ID)\n",
    "        await call_agent_async(query = \"Thanks, bye!\",\n",
    "                               runner=runner_agent_team,\n",
    "                               user_id=USER_ID,\n",
    "                               session_id=SESSION_ID)\n",
    "\n",
    "    # --- Execute the `run_team_conversation` async function ---\n",
    "    # Choose ONE of the methods below based on your environment.\n",
    "    # Note: This may require API keys for the models used!\n",
    "\n",
    "    # METHOD 1: Direct await (Default for Notebooks/Async REPLs)\n",
    "    # If your environment supports top-level await (like Colab/Jupyter notebooks),\n",
    "    # it means an event loop is already running, so you can directly await the function.\n",
    "    print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
    "    await run_team_conversation()\n",
    "\n",
    "    # METHOD 2: asyncio.run (For Standard Python Scripts [.py])\n",
    "    # If running this code as a standard Python script from your terminal,\n",
    "    # the script context is synchronous. `asyncio.run()` is needed to\n",
    "    # create and manage an event loop to execute your async function.\n",
    "    # To use this method:\n",
    "    # 1. Comment out the `await run_team_conversation()` line above.\n",
    "    # 2. Uncomment the following block:\n",
    "    \"\"\"\n",
    "    import asyncio\n",
    "    if __name__ == \"__main__\": # Ensures this runs only when script is executed directly\n",
    "        print(\"Executing using 'asyncio.run()' (for standard Python scripts)...\")\n",
    "        try:\n",
    "            # This creates an event loop, runs your async function, and closes the loop.\n",
    "            asyncio.run(run_team_conversation())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \"\"\"\n",
    "\n",
    "else:\n",
    "    # This message prints if the root agent variable wasn't found earlier\n",
    "    print(\"\\n⚠️ Skipping agent team conversation execution as the root agent was not successfully defined in a previous step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f602c773",
   "metadata": {},
   "source": [
    "#### Step 4: Adding Memory and Personalization with Session State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070264a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course_agentic_ai_venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
